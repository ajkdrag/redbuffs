---
{"publish":true,"tags":["status/done","type/topic"],"path":"3 Topics/GPT.md","permalink":"/3-topics/gpt/","PassFrontmatter":true}
---


Topics: [[LLM\|LLM]] | [[decoder-only models\|decoder-only models]]
Links: [[BERT\|BERT]]

GPT, an acronym for Generative Pre-trained Transformer, introduced in 2018, represents a cutting-edge model in Natural Language Processing (NLP). It operates as a [[3 Topics/transformer\|transformer]]-based architecture, designed for understanding and generating coherent text through [[self-attention\|self-attention]] mechanism. GPT's fundamental principle lies in [[pretraining\|pretraining]] on extensive text corpora, enabling versatile applicability in various NLP tasks.