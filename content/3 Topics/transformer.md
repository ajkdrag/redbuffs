---
{"publish":true,"tags":["status/done","type/topic"],"path":"3 Topics/transformer.md","permalink":"/3-topics/transformer/","PassFrontmatter":true}
---



> [!Topics]
> - [[LLM\|LLM]]

A groundbreaking architecture in deep learning, introduced in 2017 by Vaswani et al. It has revolutionized natural language processing and has found applications in various domains *beyond text processing*.

At its core, the Transformer relies on [[attention mechanism\|attention mechanism]] to process input data. Unlike previous models that processed sequences step by step, Transformers can look at an entire sequence at once. This allows them to capture long-range dependencies more effectively.

The model consists of two main parts: an encoder and a decoder. The encoder processes the input sequence, while the decoder generates the output. Transformers have several advantages over previous architectures:

- They can handle long sequences more effectively.
- They allow for more parallelization, making training faster.
- They have shown superior performance on many language tasks.

Despite their power, Transformers also have limitations. They can be computationally expensive, especially for very long sequences and are data hungry.

## Related
- [[BERT\|BERT]]
- [[3 Topics/GPT\|GPT]]
