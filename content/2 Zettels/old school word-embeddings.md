---
{"publish":true,"tags":["status/done","type/zettel"],"path":"2 Zettels/old school word-embeddings.md","permalink":"/2-zettels/old-school-word-embeddings/","PassFrontmatter":true}
---



> [!Topics]
> - [[word embeddings\|word embeddings]]
> - [[linear algebra\|linear algebra]]

Before [[3 Topics/word2vec\|word2vec]] (neural-like methods), word embeddings were based on co-occurrence counts. The matrix was constructred, followed by some [[matrix factorization\|matrix factorization]] approach to get lower-dimensional word embeddings.
**Advantages**
- Very fast
- Captured global statistics that word2vec misses

> [!note]
> Despite the advantages (especially in speed), word2vec just works better normally.

## Related
- [[GloVe\|GloVe]]
